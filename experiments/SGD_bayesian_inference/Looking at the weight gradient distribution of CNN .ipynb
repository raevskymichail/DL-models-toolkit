{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import util\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pystan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "We are using MNIST data for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist.train._images.reshape(55000,28,28)\n",
    "Y = mnist.train._labels\n",
    "index = np.arange(55000)\n",
    "np.random.shuffle(index)\n",
    "\n",
    "bf = util.BatchFeeder(X[index[:54000]], Y[index[:54000]])\n",
    "valid_data = (X[index[54000:]], Y[index[54000:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CNN model\n",
    "We use two layer CNN with 16 and 32 kenerls of sizes 3 by 3, followed by a dense layer with 10 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(D, N):\n",
    "    D = D*1.0\n",
    "    N = N*1.0\n",
    "    \n",
    "    #resetting parameters\n",
    "    util.reset_graph()\n",
    "    \n",
    "    # Input and output\n",
    "    # Use Placeholder in tf for input and output.\n",
    "    x_tf = tf.placeholder(tf.float64, [None, wine_itr.d])  # data matrix\n",
    "    y_tf = tf.placeholder(tf.float64, [None, 1]) # target matrix\n",
    "    lambda_tf = tf.Variable(1.0, name=\"h\", dtype=tf.float64) #tf.Variable(1.0, name=\"h\")\n",
    "    learning_rate = tf.placeholder(tf.float64)\n",
    "    \n",
    "    # Single fully connected layer\n",
    "    # Loads in the noise + optimal weights calculated above as a starting point\n",
    "    W_tf = tf.Variable(W, name=\"w\") # weight matrix\n",
    "    dense = tf.matmul(x_tf, W_tf) # fully connected multiplication\n",
    "    \n",
    "    # Objective function. This needs to be absolutely correct.\n",
    "    cost = tf.reduce_mean(tf.square(tf.subtract(y_tf, dense)))/2.0\\\n",
    "        +lambda_tf/2.0*tf.reduce_sum(tf.square(W_tf))/N\\\n",
    "        -D/2.0*tf.log(lambda_tf)/N\n",
    "    \n",
    "    # Optimization operations\n",
    "    # SGD of learning rate 1 is used because we can manually \n",
    "    # multiply gradients with custom learning rate later.\n",
    "    optimizer_sgd = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    # Get trainable variables\n",
    "    tvars = tf.trainable_variables()\n",
    "    wvars = [i for i in tvars if i._variable.name[0] == \"w\"]\n",
    "    hvars = [i for i in tvars if i._variable.name[0] == \"h\"]\n",
    "    \n",
    "    # Get gradients with respect to parameters\n",
    "    w_gradients = optimizer_sgd.compute_gradients(cost, wvars)\n",
    "    h_gradients = optimizer_sgd.compute_gradients(cost, hvars)\n",
    "\n",
    "    # Learing rates holder. This is a tuple of placeholders that\n",
    "    # match the size of the parameters in the variable holders.\n",
    "    precision_holders = tuple(tf.placeholder(tf.float64, [np.prod([i.value for i in g[0].shape]),\\\n",
    "                        np.prod([i.value for i in g[0].shape])]) for g in w_gradients)\n",
    "    \n",
    "    # Calculating a sgd step given prescition matrix.\n",
    "    sgd_steps = [(tf.matmul(precision_holders[i], w_gradients[i][0]),\\\n",
    "                           w_gradients[i][1]) for i in range(len(w_gradients))]\n",
    "    \n",
    "    # Applying it to current weights.\n",
    "    train_w = optimizer_sgd.apply_gradients(sgd_steps)\n",
    "    train_h = optimizer_sgd.apply_gradients(h_gradients)\n",
    "    \n",
    "    return dict(\n",
    "        x = x_tf,\n",
    "        y = y_tf,\n",
    "        _lambda = lambda_tf,\n",
    "        lr = learning_rate,\n",
    "        w_gradients = w_gradients,\n",
    "        h_gradients = h_gradients,\n",
    "        train_w = train_w,\n",
    "        train_h = train_h,\n",
    "        current_w = W_tf,\n",
    "        precision_matrix = precision_holders\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
